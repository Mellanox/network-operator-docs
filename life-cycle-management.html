<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Life Cycle Management &mdash; NVIDIA Network Operator v25.04.0 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" type="text/css" />
      <link rel="stylesheet" href="_static/omni-style.css" type="text/css" />
      <link rel="stylesheet" href="_static/api-styles.css" type="text/css" />
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/mermaid-init.js"></script>
        <script src="_static/design-tabs.js"></script>
        <script src="_static/version.js"></script>
        <script src="_static/social-media.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Advanced Configurations" href="advanced/advanced.html" />
    <link rel="prev" title="NicClusterPolicy Custom Resource Example" href="customizations/cr-full-example.html" />
 


</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >


<a href="index.html">
  <img src="_static/nvidia-logo-white.png" class="logo" alt="Logo"/>
</a>

<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">NVIDIA Network Operator</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="release-notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="platform-support.html">Platform Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="getting-started-kubernetes.html">Getting Started with Kubernetes</a></li>
<li class="toctree-l1"><a class="reference internal" href="getting-started-openshift.html">Getting Started with Red Hat OpenShift</a></li>
<li class="toctree-l1"><a class="reference internal" href="customizations/customization.html">Customization Options and CRDs</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Life Cycle Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="advanced/advanced.html">Advanced Configurations</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">NVIDIA Network Operator</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">


<li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
  
<li>Life Cycle Management</li>

      <li class="wy-breadcrumbs-aside">
      </li>
<li class="wy-breadcrumbs-aside">

  <span>&nbsp;</span>
</li>

  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="life-cycle-management">
<h1>Life Cycle Management<a class="headerlink" href="#life-cycle-management" title="Permalink to this headline"></a></h1>
<div class="contents local topic" id="on-this-page">
<p class="topic-title">On this page</p>
<ul class="simple">
<li><p><a class="reference internal" href="#network-operator-versioning" id="id2">Network Operator Versioning</a></p></li>
<li><p><a class="reference internal" href="#network-operator-life-cycle" id="id3">Network Operator Life Cycle</a></p></li>
<li><p><a class="reference internal" href="#ensuring-deployment-readiness" id="id4">Ensuring Deployment Readiness</a></p>
<ul>
<li><p><a class="reference internal" href="#status-field-example-of-a-nicclusterpolicy-instance" id="id5">Status Field Example of a NICClusterPolicy Instance</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#network-operator-upgrade" id="id6">Network Operator Upgrade</a></p>
<ul>
<li><p><a class="reference internal" href="#downloading-a-new-helm-chart" id="id7">Downloading a New Helm Chart</a></p></li>
<li><p><a class="reference internal" href="#upgrading-crds-for-a-specific-release" id="id8">Upgrading CRDs for a Specific Release</a></p></li>
<li><p><a class="reference internal" href="#preparing-the-helm-values-for-the-new-release" id="id9">Preparing the Helm Values for the New Release</a></p></li>
<li><p><a class="reference internal" href="#applying-the-helm-chart-update" id="id10">Applying the Helm Chart Update</a></p></li>
<li><p><a class="reference internal" href="#doca-driver-manual-upgrade" id="id11">DOCA Driver Manual Upgrade</a></p>
<ul>
<li><p><a class="reference internal" href="#restarting-pods-with-a-containerized-doca-driver" id="id12">Restarting Pods with a Containerized DOCA Driver</a></p></li>
<li><p><a class="reference internal" href="#removing-pods-with-a-secondary-network-from-the-node" id="id13">Removing Pods with a Secondary Network from the Node</a></p></li>
<li><p><a class="reference internal" href="#restarting-the-doca-driver-pod" id="id14">Restarting the DOCA Driver Pod</a></p></li>
<li><p><a class="reference internal" href="#deleting-the-doca-driver-pod-from-the-node" id="id15">Deleting the DOCA Driver Pod from the Node</a></p>
<ul>
<li><p><a class="reference internal" href="#returning-pods-with-a-secondary-network-to-the-node" id="id16">Returning Pods with a Secondary Network to the Node</a></p></li>
</ul>
</li>
</ul>
</li>
<li><p><a class="reference internal" href="#automatic-doca-driver-upgrade" id="id17">Automatic DOCA  Driver Upgrade</a></p>
<ul>
<li><p><a class="reference internal" href="#node-upgrade-states" id="id18">Node Upgrade States</a></p></li>
<li><p><a class="reference internal" href="#safe-driver-loading" id="id19">Safe Driver Loading</a></p>
<ul>
<li><p><a class="reference internal" href="#troubleshooting" id="id20">Troubleshooting</a></p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><a class="reference internal" href="#uninstalling-the-network-operator" id="id21">Uninstalling the Network Operator</a></p>
<ul>
<li><p><a class="reference internal" href="#uninstalling-network-operator-on-a-vanilla-kubernetes-cluster" id="id22">Uninstalling Network Operator on a Vanilla Kubernetes Cluster</a></p></li>
<li><p><a class="reference internal" href="#uninstalling-the-network-operator-on-an-openshift-cluster" id="id23">Uninstalling the Network Operator on an OpenShift Cluster</a></p></li>
<li><p><a class="reference internal" href="#additional-steps" id="id24">Additional Steps</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#nicclusterpolicy-crd-update" id="id25">NicClusterPolicy CRD Update</a></p></li>
</ul>
</div>
<section id="network-operator-versioning">
<h2>Network Operator Versioning<a class="headerlink" href="#network-operator-versioning" title="Permalink to this headline"></a></h2>
<p>NVIDIA Network Operator is versioned following the calendar versioning convention.</p>
<p>The version follows the pattern <code class="docutils literal notranslate"><span class="pre">YY.MM.PP</span></code>, such as 25.1.0, 24.10.0, and 24.10.1.
The first two fields, <code class="docutils literal notranslate"><span class="pre">YY.MM</span></code> identify a major version and indicates when the major version was initially released.
The third field, <code class="docutils literal notranslate"><span class="pre">PP</span></code>, identifies the patch version of the major version.
Patch releases typically include critical bug and CVE fixes.</p>
</section>
<section id="network-operator-life-cycle">
<h2>Network Operator Life Cycle<a class="headerlink" href="#network-operator-life-cycle" title="Permalink to this headline"></a></h2>
<p>When a major version of NVIDIA Network Operator is released, the previous major version enters maintenance support
and only receives patch release updates for critical bug and CVE fixes.
Prior maintance release enters EOL (end-of-life) and are no longer supported and do not receive patch release updates.</p>
<p>The product life cycle and versioning are subject to change in the future.</p>
<table class="docutils align-default" id="id1">
<caption><span class="caption-text">Support Status for Releases</span><a class="headerlink" href="#id1" title="Permalink to this table"></a></caption>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Network Operator Version</p></th>
<th class="head"><p>Status</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>25.1.x</p></td>
<td><p>Generally Available</p></td>
</tr>
<tr class="row-odd"><td><p>24.10.x</p></td>
<td><p>Maintenance</p></td>
</tr>
<tr class="row-even"><td><p>24.7.x and lower</p></td>
<td><p>EOL</p></td>
</tr>
</tbody>
</table>
</section>
<section id="ensuring-deployment-readiness">
<h2>Ensuring Deployment Readiness<a class="headerlink" href="#ensuring-deployment-readiness" title="Permalink to this headline"></a></h2>
<p>Once the Network Operator is deployed, and a NicClusterPolicy resource is created, the operator will reconcile the state of the cluster until it reaches the desired state, as defined in the resource.</p>
<p>Alignment of the cluster to the defined policy can be verified in the custom resource status.</p>
<p>a “Ready” state indicates that the required components were deployed, and that the policy is applied on the cluster.</p>
<section id="status-field-example-of-a-nicclusterpolicy-instance">
<h3>Status Field Example of a NICClusterPolicy Instance<a class="headerlink" href="#status-field-example-of-a-nicclusterpolicy-instance" title="Permalink to this headline"></a></h3>
<p>Get the NicClusterPolicy status:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>kubectl get -n nvidia-network-operator nicclusterpolicies.mellanox.com nic-cluster-policy -o yaml
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>status:
  appliedStates:
  - name: state-pod-security-policy
    state: ignore
  - name: state-multus-cni
    state: ready
  - name: state-container-networking-plugins
    state: ignore
  - name: state-ipoib-cni
    state: ignore
  - name: state-whereabouts-cni
    state: ready
  - name: state-OFED
    state: ready
  - name: state-SRIOV-device-plugin
    state: ignore
  - name: state-RDMA-device-plugin
    state: ready
  - name: state-NV-Peer
    state: ignore
  - name: state-ib-kubernetes
    state: ignore
  - name: state-nv-ipam-cni
    state: ready
  state: ready
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>An “Ignore” state indicates that the sub-state was not defined in the custom resource, and thus, it is ignored.</p>
</div>
</section>
</section>
<section id="network-operator-upgrade">
<h2>Network Operator Upgrade<a class="headerlink" href="#network-operator-upgrade" title="Permalink to this headline"></a></h2>
<p>Before upgrading to Network Operator v24.10 or newer with SR-IOV Network Operator enabled, the following manual actions are required:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ kubectl -n nvidia-network-operator scale deployment network-operator-sriov-network-operator --replicas <span class="m">0</span>

$ kubectl -n nvidia-network-operator delete sriovnetworknodepolicies.sriovnetwork.openshift.io default
</pre></div>
</div>
<p>The network operator provides limited upgrade capabilities, which require additional manual actions if a containerized DOCA Driver is used. Future releases of the network operator will provide an automatic upgrade flow for the containerized driver.</p>
<p>Since Helm does not support auto-upgrade of existing CRDs, the user must follow a two-step process to upgrade the network-operator release:</p>
<ul class="simple">
<li><p>Upgrade the CRD to the latest version</p></li>
<li><p>Apply Helm chart update</p></li>
</ul>
<section id="downloading-a-new-helm-chart">
<h3>Downloading a New Helm Chart<a class="headerlink" href="#downloading-a-new-helm-chart" title="Permalink to this headline"></a></h3>
<p>To obtain new releases, run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span> <span class="c1"># Download Helm chart</span>
 $ helm fetch <span class="se">\h</span>ttps://helm.ngc.nvidia.com/nvidia/charts/network-operator-25.1.0.tgz
 $ ls network-operator-<span class="se">\*</span>.tgz <span class="p">|</span> xargs -n <span class="m">1</span> tar xf
</pre></div>
</div>
</section>
<section id="upgrading-crds-for-a-specific-release">
<h3>Upgrading CRDs for a Specific Release<a class="headerlink" href="#upgrading-crds-for-a-specific-release" title="Permalink to this headline"></a></h3>
<p>It is possible to retrieve updated CRDs from the Helm chart or from the release branch on GitHub. The example below shows how to upgrade CRDs from the downloaded chart.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ kubectl apply <span class="se">\</span>
  -f network-operator/crds <span class="se">\</span>
  -f network-operator/charts/sriov-network-operator/crds
</pre></div>
</div>
</section>
<section id="preparing-the-helm-values-for-the-new-release">
<h3>Preparing the Helm Values for the New Release<a class="headerlink" href="#preparing-the-helm-values-for-the-new-release" title="Permalink to this headline"></a></h3>
<p>Edit the values-&lt;VERSION&gt;.yaml file as required for your cluster. The network operator has some limitations as to which updates in the NicClusterPolicy it can handle automatically. If the configuration for the new release is different from the current configuration in the deployed release, some additional manual actions may be required.</p>
<p>Known limitations:</p>
<ul class="simple">
<li><p>If component configuration was removed from the NicClusterPolicy, manual clean up of the component’s resources (DaemonSets, ConfigMaps, etc.) may be required.</p></li>
<li><p>If the configuration for devicePlugin changed without image upgrade, manual restart of the devicePlugin may be required.</p></li>
</ul>
<p>These limitations will be addressed in future releases.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Changes that were made directly in the NicClusterPolicy CR (e.g. with kubectl edit) will be overwritten by the Helm upgrade due to the <cite>force</cite> flag.</p>
</div>
</section>
<section id="applying-the-helm-chart-update">
<h3>Applying the Helm Chart Update<a class="headerlink" href="#applying-the-helm-chart-update" title="Permalink to this headline"></a></h3>
<p>To apply the Helm chart update, run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ helm upgrade -n nvidia-network-operator network-operator nvidia/network-operator --version<span class="o">=</span>&lt;VERSION&gt; -f values-&lt;VERSION&gt;.yaml --force
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The –devel option is required if you wish to use the Beta release.</p>
</div>
</section>
<section id="doca-driver-manual-upgrade">
<h3>DOCA Driver Manual Upgrade<a class="headerlink" href="#doca-driver-manual-upgrade" title="Permalink to this headline"></a></h3>
<section id="restarting-pods-with-a-containerized-doca-driver">
<h4>Restarting Pods with a Containerized DOCA Driver<a class="headerlink" href="#restarting-pods-with-a-containerized-doca-driver" title="Permalink to this headline"></a></h4>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This operation is required only if containerized DOCA Driver is in use.</p>
</div>
<p>When a containerized DOCA Driver is reloaded on the node, all pods that use a secondary network based on NVIDIA NICs will lose network interface in their containers. To prevent outage, remove all pods that use a secondary network from the node before you reload the driver pod on it.</p>
<p>The Helm upgrade command will only upgrade the DaemonSet spec of the DOCA Driver to point to the new driver version. The DOCA Driver’s DaemonSet will not automatically restart pods with the driver on the nodes, as it uses “OnDelete” updateStrategy. The old DOCA Driver version will still run on the node until you explicitly remove the driver pod or reboot the node:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ kubectl delete pod -l <span class="nv">app</span><span class="o">=</span>mofed-&lt;OS_NAME&gt; -n nvidia-network-operator
</pre></div>
</div>
<p>It is possible to remove all pods with secondary networks from all cluster nodes, and then restart the DOCA Driver pods on all nodes at once.</p>
<p>The alternative option is to perform an upgrade in a rolling manner to reduce the impact of the driver upgrade on the cluster. The driver pod restart can be done on each node individually. In this case, pods with secondary networks should be removed from the single node only. There is no need to stop pods on all nodes.</p>
<p>For each node, follow these steps to reload the driver on the node:</p>
<ol class="arabic simple">
<li><p>Remove pods with a secondary network from the node.</p></li>
<li><p>Restart the DOCA Driver pod.</p></li>
<li><p>Return the pods with a secondary network to the node.</p></li>
</ol>
<p>When the DOCA Driver is ready, proceed with the same steps for other nodes.</p>
</section>
<section id="removing-pods-with-a-secondary-network-from-the-node">
<h4>Removing Pods with a Secondary Network from the Node<a class="headerlink" href="#removing-pods-with-a-secondary-network-from-the-node" title="Permalink to this headline"></a></h4>
<p>To remove pods with a secondary network from the node with node drain, run the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ kubectl drain &lt;NODE_NAME&gt; --pod-selector<span class="o">=</span>&lt;SELECTOR_FOR_PODS&gt;
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Replace &lt;NODE_NAME&gt; with -l “network.nvidia.com/operator.mofed.wait=false” if you wish to drain all nodes at once.</p>
</div>
</section>
<section id="restarting-the-doca-driver-pod">
<h4>Restarting the DOCA Driver Pod<a class="headerlink" href="#restarting-the-doca-driver-pod" title="Permalink to this headline"></a></h4>
<p>Find the DOCA Driver pod name for the node:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ kubectl get pod -l <span class="nv">app</span><span class="o">=</span>mofed-&lt;OS_NAME&gt; -o wide -A
</pre></div>
</div>
<p>Example for Ubuntu 20.04:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>kubectl get pod -l <span class="nv">app</span><span class="o">=</span>mofed-ubuntu20.04 -o wide -A
</pre></div>
</div>
</section>
<section id="deleting-the-doca-driver-pod-from-the-node">
<h4>Deleting the DOCA Driver Pod from the Node<a class="headerlink" href="#deleting-the-doca-driver-pod-from-the-node" title="Permalink to this headline"></a></h4>
<p>To delete the DOCA Driver pod from the node, run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ kubectl delete pod -n &lt;DRIVER_NAMESPACE&gt; &lt;DOCA_DRIVER_POD_NAME&gt;
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Replace &lt;DOCA_DRIVER_POD_NAME&gt; with -l app=mofed-ubuntu20.04 if you wish to remove DOCA Driver pods on all nodes at once.</p>
</div>
<p>A new version of the DOCA Driver pod will automatically start.</p>
<section id="returning-pods-with-a-secondary-network-to-the-node">
<h5>Returning Pods with a Secondary Network to the Node<a class="headerlink" href="#returning-pods-with-a-secondary-network-to-the-node" title="Permalink to this headline"></a></h5>
<p>After the DOCA Driver pod is ready on the node, you can make the node schedulable again.</p>
<p>The command below will uncordon (remove node.kubernetes.io/unschedulable:NoSchedule taint) the node, and return the pods to it:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ kubectl uncordon -l <span class="s2">&quot;network.nvidia.com/operator.mofed.wait=false&quot;</span>
</pre></div>
</div>
</section>
</section>
</section>
<section id="automatic-doca-driver-upgrade">
<h3>Automatic DOCA  Driver Upgrade<a class="headerlink" href="#automatic-doca-driver-upgrade" title="Permalink to this headline"></a></h3>
<p>To enable automatic DOCA Driver upgrade, define the UpgradePolicy section for the ofedDriver in the NicClusterPolicy spec, and change the DOCA Driver version.</p>
<p><code class="docutils literal notranslate"><span class="pre">nicclusterpolicy.yaml</span></code>:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="w"> </span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">mellanox.com/v1alpha1</span><span class="w"></span>
<span class="w"> </span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">NicClusterPolicy</span><span class="w"></span>
<span class="w"> </span><span class="nt">metadata</span><span class="p">:</span><span class="w"></span>
<span class="w">   </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nic-cluster-policy</span><span class="w"></span>
<span class="w">   </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvidia-network-operator</span><span class="w"></span>
<span class="w"> </span><span class="nt">spec</span><span class="p">:</span><span class="w"></span>
<span class="w">   </span><span class="nt">ofedDriver</span><span class="p">:</span><span class="w"></span>
<span class="w">     </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">doca-driver</span><span class="w"></span>
<span class="w">     </span><span class="nt">repository</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvcr.io/nvidia/mellanox</span><span class="w"></span>
<span class="w">     </span><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">25.01-0.6.0.0-0</span><span class="w"></span>
<span class="w">     </span><span class="nt">upgradePolicy</span><span class="p">:</span><span class="w"></span>
<span class="w">       </span><span class="c1"># autoUpgrade is a global switch for automatic upgrade feature</span><span class="w"></span>
<span class="w">       </span><span class="c1"># if set to false all other options are ignored</span><span class="w"></span>
<span class="w">       </span><span class="nt">autoUpgrade</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span><span class="w"></span>
<span class="w">       </span><span class="c1"># maxParallelUpgrades indicates how many nodes can be upgraded in parallel</span><span class="w"></span>
<span class="w">       </span><span class="c1"># 0 means no limit, all nodes will be upgraded in parallel</span><span class="w"></span>
<span class="w">       </span><span class="nt">maxParallelUpgrades</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span><span class="w"></span>
<span class="w">       </span><span class="c1"># cordon and drain (if enabled) a node before loading the driver on it</span><span class="w"></span>
<span class="w">       </span><span class="nt">safeLoad</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span><span class="w"></span>
<span class="w">       </span><span class="c1"># describes the configuration for waiting on job completions</span><span class="w"></span>
<span class="w">       </span><span class="nt">waitForCompletion</span><span class="p">:</span><span class="w"></span>
<span class="w">         </span><span class="c1"># specifies a label selector for the pods to wait for completion</span><span class="w"></span>
<span class="w">         </span><span class="nt">podSelector</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;app=myapp&quot;</span><span class="w"></span>
<span class="w">         </span><span class="c1"># specify the length of time in seconds to wait before giving up for workload to finish, zero means infinite</span><span class="w"></span>
<span class="w">         </span><span class="c1"># if not specified, the default is 300 seconds</span><span class="w"></span>
<span class="w">         </span><span class="nt">timeoutSeconds</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">300</span><span class="w"></span>
<span class="w">       </span><span class="c1"># describes configuration for node drain during automatic upgrade</span><span class="w"></span>
<span class="w">       </span><span class="nt">drain</span><span class="p">:</span><span class="w"></span>
<span class="w">         </span><span class="c1"># allow node draining during upgrade</span><span class="w"></span>
<span class="w">         </span><span class="nt">enable</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span><span class="w"></span>
<span class="w">         </span><span class="c1"># allow force draining</span><span class="w"></span>
<span class="w">         </span><span class="nt">force</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span><span class="w"></span>
<span class="w">         </span><span class="c1"># specify a label selector to filter pods on the node that need to be drained</span><span class="w"></span>
<span class="w">         </span><span class="nt">podSelector</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="w"></span>
<span class="w">         </span><span class="c1"># specify the length of time in seconds to wait before giving up drain, zero means infinite</span><span class="w"></span>
<span class="w">         </span><span class="c1"># if not specified, the default is 300 seconds</span><span class="w"></span>
<span class="w">         </span><span class="nt">timeoutSeconds</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">300</span><span class="w"></span>
<span class="w">         </span><span class="c1"># specify if should continue even if there are pods using emptyDir</span><span class="w"></span>
<span class="w">         </span><span class="nt">deleteEmptyDir</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span><span class="w"></span>
</pre></div>
</div>
<p>Apply NicClusterPolicy CRD:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ kubectl apply -f nicclusterpolicy.yaml
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>To be able to drain nodes, make sure to fill the PodDisruptionBudget field for all the pods that use it. On some clusters (e.g. Openshift), many pods use PodDisruptionBudget, which makes draining multiple nodes at once impossible. Since evicting several pods that are controlled by the same deployment or replica set, violates their PodDisruptionBudget, those pods are not evicted and in drain failure.</p>
<p>To perform a driver upgrade, the network-operator must evict pods that are using network resources. Therefore, in order to ensure that the network-operator is evicting only the required pods, the upgradePolicy.drain.podSelector field must be configured.</p>
</div>
<section id="node-upgrade-states">
<h4>Node Upgrade States<a class="headerlink" href="#node-upgrade-states" title="Permalink to this headline"></a></h4>
<p>The status upgrade of each node is reflected in its nvidia.com/ofed-driver-upgrade-state label . This label can have the following values:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Unknown (empty)</p></td>
<td><p>The node has this state when the upgrade flow is disabled or the node has not been processed yet.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">upgrade-done</span></code></p></td>
<td><p>Set when DOCA Driver POD is up-to-date and running on the node, the node is schedulable.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">upgrade-required</span></code></p></td>
<td><p>Set when DOCA Driver POD on the node is not up-to-date and requires upgrade. No actions are performed at this stage.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">cordon-required</span></code></p></td>
<td><p>Set when the node needs to be made unschedulable in preparation for driver upgrade.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">wait-for-jobs-required</span></code></p></td>
<td><p>Set on the node when waiting is required for jobs to complete until the given timeout.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">drain-required</span></code></p></td>
<td><p>Set when the node is scheduled for drain. After the drain, the state is changed either to pod-restart-required or upgrade-failed.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">pod-restart-required</span></code></p></td>
<td><p>Set when the DOCA Driver POD on the node is scheduled for restart. After the restart, the state is changed to uncordon-required.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">uncordon-required</span></code></p></td>
<td><p>Set when DOCA Driver POD on the node is up-to-date and has “Ready” status. After uncordone, the state is changed to upgrade-done</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">upgrade-failed</span></code></p></td>
<td><p>Set when the upgrade on the node has failed. Manual interaction is required at this stage. See Troubleshooting section for more details.</p></td>
</tr>
</tbody>
</table>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Depending on your cluster workloads and pod Disruption Budget, set the following values for auto upgrade:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">mellanox.com/v1alpha1</span><span class="w"></span>
<span class="w">  </span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">NicClusterPolicy</span><span class="w"></span>
<span class="w">  </span><span class="nt">metadata</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nic-cluster-policy</span><span class="w"></span>
<span class="w">    </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvidia-network-operator</span><span class="w"></span>
<span class="w">  </span><span class="nt">spec</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">ofedDriver</span><span class="p">:</span><span class="w"></span>
<span class="w">      </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">doca-driver</span><span class="w"></span>
<span class="w">      </span><span class="nt">repository</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvcr.io/nvidia/mellanox</span><span class="w"></span>
<span class="w">      </span><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">25.01-0.6.0.0-0</span><span class="w"></span>
<span class="w">      </span><span class="nt">upgradePolicy</span><span class="p">:</span><span class="w"></span>
<span class="w">        </span><span class="nt">autoUpgrade</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span><span class="w"></span>
<span class="w">        </span><span class="nt">maxParallelUpgrades</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w"></span>
<span class="w">        </span><span class="nt">drain</span><span class="p">:</span><span class="w"></span>
<span class="w">          </span><span class="nt">enable</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span><span class="w"></span>
<span class="w">          </span><span class="nt">force</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span><span class="w"></span>
<span class="w">          </span><span class="nt">deleteEmptyDir</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span><span class="w"></span>
<span class="w">          </span><span class="nt">podSelector</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="w"></span>
</pre></div>
</div>
</div>
</section>
<section id="safe-driver-loading">
<h4>Safe Driver Loading<a class="headerlink" href="#safe-driver-loading" title="Permalink to this headline"></a></h4>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The state of this feature can be controlled with the ofedDriver.upgradePolicy.safeLoad option.</p>
</div>
<p>Upon node startup, the DOCA Driver container takes some time to compile and load the driver. During that time, workloads might get scheduled on that node. When DOCA Driver is loaded, all existing PODs that use NVIDIA NICs will lose their network interfaces. Some such PODs might silently fail or hang. To avoid this situation, before the DOCA Driver container is loaded, the node should get cordoned and drained to ensure all workloads are rescheduled. The node should be un-cordoned when the driver is ready on it.</p>
<p>The safe driver loading feature is implemented as a part of the upgrade flow, meaning safe driver loading is a special scenario of the upgrade procedure, where we upgrade from the inbox driver to the containerized DOCA Driver.</p>
<p>When this feature is enabled, the initial DOCA Driver driver rollout on the large cluster can take a while. To speed up the rollout, the initial deployment can be done with the safe driver loading feature disabled, and this feature can be enabled later by updating the NicClusterPolicy CRD.</p>
<section id="troubleshooting">
<h5>Troubleshooting<a class="headerlink" href="#troubleshooting" title="Permalink to this headline"></a></h5>
<table class="docutils align-default">
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Issue</p></th>
<th class="head"><p>Required Action</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>The node is in upgrade-failed state.</p></td>
<td><ul class="simple">
<li><p>Drain the node manually by running kubectl drain &lt;node name&gt; –ignore-daemonsets.</p></li>
<li><p>Delete the NVIDIA DOCA Driver pod on the node manually, by running the following command: <code class="docutils literal notranslate"><span class="pre">kubectl</span> <span class="pre">delete</span> <span class="pre">pod</span> <span class="pre">-n</span> <span class="pre">`kubectl</span> <span class="pre">get</span> <span class="pre">pods</span> <span class="pre">--A</span> <span class="pre">--field-selector</span> <span class="pre">spec.nodeName=&lt;node</span> <span class="pre">name&gt;</span> <span class="pre">-l</span> <span class="pre">nvidia.com/ofed-driver</span> <span class="pre">--no-headers</span> <span class="pre">|</span> <span class="pre">awk</span> <span class="pre">'{print</span> <span class="pre">$1</span> <span class="pre">&quot;</span> <span class="pre">&quot;$2}'`</span></code>.</p></li>
</ul>
<p><strong>NOTE:</strong> If the “Safe driver loading” feature is enabled, you may also need to remove the <code class="docutils literal notranslate"><span class="pre">nvidia.com/ofed-driver-upgrade.driver-wait-for-safe-load</span></code> annotation from the node object to unblock the loading of the driver
<code class="docutils literal notranslate"><span class="pre">kubectl</span> <span class="pre">annotate</span> <span class="pre">node</span> <span class="pre">&lt;node_name&gt;</span> <span class="pre">nvidia.com/ofed-driver-upgrade.driver-wait-for-safe-load-</span></code></p>
<ul class="simple">
<li><p>Wait for the node to complete the upgrade.</p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p>The updated NVIDIA DOCA Driver pod failed to start/ a new version of NVIDIA DOCA Driver cannot be installed on the node.</p></td>
<td><p>Manually delete the pod by using <code class="docutils literal notranslate"><span class="pre">kubectl</span> <span class="pre">delete</span> <span class="pre">-n</span> <span class="pre">&lt;Network</span> <span class="pre">Operator</span> <span class="pre">Namespace&gt;</span> <span class="pre">&lt;pod</span> <span class="pre">name&gt;</span></code>.
If following the restart the pod still fails, change the NVIDIA DOCA Driver version in the NicClusterPolicy to the previous version or to another working version.</p></td>
</tr>
</tbody>
</table>
</section>
</section>
</section>
</section>
<section id="uninstalling-the-network-operator">
<h2>Uninstalling the Network Operator<a class="headerlink" href="#uninstalling-the-network-operator" title="Permalink to this headline"></a></h2>
<section id="uninstalling-network-operator-on-a-vanilla-kubernetes-cluster">
<h3>Uninstalling Network Operator on a Vanilla Kubernetes Cluster<a class="headerlink" href="#uninstalling-network-operator-on-a-vanilla-kubernetes-cluster" title="Permalink to this headline"></a></h3>
<p>Delete the NicClusterPolicy:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>kubectl delete -n nvidia-network-operator nicclusterpolicies.mellanox.com nic-cluster-policy
</pre></div>
</div>
<p>Uninstall the Network Operator:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>helm uninstall network-operator -n nvidia-network-operator
</pre></div>
</div>
<p>You should now see all the pods being deleted:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>kubectl get pods -n nvidia-network-operator
</pre></div>
</div>
<p>Make sure that the CRDs created during the operator installation have been removed:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>kubectl get nicclusterpolicies.mellanox.com
No resources found
</pre></div>
</div>
</section>
<section id="uninstalling-the-network-operator-on-an-openshift-cluster">
<h3>Uninstalling the Network Operator on an OpenShift Cluster<a class="headerlink" href="#uninstalling-the-network-operator-on-an-openshift-cluster" title="Permalink to this headline"></a></h3>
<p>From the console:</p>
<p>In the OpenShift Container Platform web console side menu, select <strong>Operators &gt;Installed Operators</strong>, search for the <strong>NVIDIA Network Operator</strong>, and click on it.</p>
<p>On the right side of the <strong>Operator Details</strong> page, select <strong>Uninstall Operator</strong> from the <strong>Actions</strong> drop-down menu.</p>
<p>For additional information, see the <a class="reference external" href="https://docs.openshift.com/container-platform/4.10/operators/admin/olm-deleting-operators-from-cluster.html">Red Hat OpenShift Container Platform Documentation</a>.</p>
<p>From the CLI:</p>
<blockquote>
<div><ul>
<li><p>Check the current version of the Network Operator in the currentCSV field:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>oc get subscription -n nvidia-network-operator nvidia-network-operator -o yaml <span class="p">|</span> grep currentCSV
</pre></div>
</div>
<p>Example output:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>currentCSV: nvidia-network-operator.v24.1.0
</pre></div>
</div>
</li>
<li><p>Delete the subscription:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>oc delete subscription -n nvidia-network-operator nvidia-network-operator
</pre></div>
</div>
<p>Example output:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>subscription.operators.coreos.com <span class="s2">&quot;nvidia-network-operator&quot;</span> deleted
</pre></div>
</div>
</li>
<li><p>Delete the CSV using the currentCSV value from the previous step:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>subscription.operators.coreos.com <span class="s2">&quot;nvidia-network-operator&quot;</span> deleted
</pre></div>
</div>
<p>Example output:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>clusterserviceversion.operators.coreos.com <span class="s2">&quot;nvidia-network-operator.v10.0&quot;</span> deleted
</pre></div>
</div>
</li>
</ul>
</div></blockquote>
<p>The SR-IOV Network Operator uninstallation procedure is described in this document. For additional information, see the <a class="reference external" href="https://docs.openshift.com/container-platform/4.10/operators/admin/olm-deleting-operators-from-cluster.html">Red Hat OpenShift Container Platform Documentation</a>.</p>
</section>
<section id="additional-steps">
<h3>Additional Steps<a class="headerlink" href="#additional-steps" title="Permalink to this headline"></a></h3>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>In OCP, uninstalling an operator does not remove its managed resources, including CRDs and CRs. To remove them, you must manually delete the Operator CRDs following the operator uninstallation.</p>
</div>
<p>Delete the Network Operator CRDs:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>oc delete crds hostdevicenetworks.mellanox.com macvlannetworks.mellanox.com nicclusterpolicies.mellanox.com
</pre></div>
</div>
</section>
</section>
<section id="nicclusterpolicy-crd-update">
<h2>NicClusterPolicy CRD Update<a class="headerlink" href="#nicclusterpolicy-crd-update" title="Permalink to this headline"></a></h2>
<p>If the NicClusterPolicy manual update affects the device plugin configuration (e.g. NICs selectors), manual device plugin pods restart is required.</p>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, NVIDIA.
      <span class="lastupdated">Last updated on Mar 24, 2025.
      </span></p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 



</body>
</html>